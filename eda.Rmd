---
title: "Exploratory Data Analysis on text"
author: "Alberto Mac√≠as"
date: "5/27/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# EDA on twitter text and predicting text

## Libraries for the analysis
```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)

library(tidytext)
library(tm) # Text mining
library(tokenizers) # Tokenize the data
library(sbo) # Predicting text models
```


## Loading data and preparing corpus
```{r, warning=FALSE}
file.txt <-  paste(getwd(), "final", "en_Us", "en_US.twitter.txt", sep = "/")
con <- file(file.txt, "r")
twitter.corpus <- readLines(con)
close(con)
set.seed(22)
selection <- sample(1:length(twitter.corpus), 100000)
twitter.corpus <- twitter.corpus[selection]
twitter_corpus <- Corpus(VectorSource(twitter.corpus),
                         readerControl=list(readPlain, language="en_US", load=TRUE))
inspect(twitter_corpus[1:4])
```

## Data cleaning

- transform to lowercase  
- remove punctuation and symbols  
- stem the corpus  
- filter profanities  
- remove stopwords 
- remove extra white spaces  
- create term document matrix and 2-gram and 3-gram matrices  

```{r}
twitter_corpus <- tm_map(twitter_corpus, content_transformer(tolower))
twitter_corpus <- tm_map(twitter_corpus, removePunctuation)
twitter_corpus <- tm_map(twitter_corpus, stemDocument)
```

```{r}
set.seed(123)
s <- sample(1:length(twitter_corpus), 1000)
profanity_list <- unique(lexicon::profanity_banned) #List containing stop words
twitter.profanity <- sentimentr::profanity(sapply(twitter_corpus[s], as.character), 
                                    profanity_list=profanity_list)
twitter.profanity %>% ggplot(aes(x=profanity))+
            geom_density()
```


```{r}
twitter_corpus <- tm_map(twitter_corpus, removeWords, profanity_list)
twitter.profanity <- sentimentr::profanity(sapply(twitter_corpus[s], as.character), 
                                    profanity_list=profanity_list)
mean(twitter.profanity$profanity)
# Create a confidence interval with bootstrap sampling
```

```{r}
twitter_corpus <- tm_map(twitter_corpus, stripWhitespace)
```


```{r}
twitter_2_grams <- tokenize_ngrams(sapply(twitter_corpus, as.character), 
                                   n=2)
twitter_2_grams <- unlist(twitter_2_grams)
twitter_2_grams[1:5]
table_2_grams <- data.frame(table(twitter_2_grams))
table_2_grams <- table_2_grams %>% arrange(desc(Freq))
head(table_2_grams)
```

```{r}
twitter_2_grams2 <- tokenize_ngrams(sapply(twitter_corpus, as.character), 
                                   n=2, stopwords = stopwords("en"))
twitter_2_grams2 <- unlist(twitter_2_grams2)
table_2_grams2 <- data.frame(table(twitter_2_grams2))
table_2_grams2 <- table_2_grams2 %>% arrange(desc(Freq))
head(table_2_grams2)
```

```{r}
twitter_3_grams <- tokenize_ngrams(sapply(twitter_corpus, as.character), 
                                   n=3, stopwords = stopwords("en"))
twitter_3_grams <- unlist(twitter_3_grams)
table_3_grams <- data.frame(table(twitter_3_grams))
table_3_grams <- table_3_grams %>% arrange(desc(Freq))
head(table_3_grams, 10)
```


```{r}
twitter_corpus <- tm_map(twitter_corpus, removeWords, stopwords("english"))
twitter_corpus <- tm_map(twitter_corpus, stripWhitespace)
inspect(twitter_corpus[1:4])
```

```{r}
dtm <- DocumentTermMatrix(twitter_corpus)
```

## EDA
