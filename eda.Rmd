---
title: "Exploratory Data Analysis on text"
author: "Alberto Macías"
date: "5/27/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# EDA on twitter text and predicting text

## Libraries for the analysis
```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(gridExtra)

library(tidytext)
library(tm) # Text mining
library(tokenizers) # Tokenize the data
library(sbo) # Predicting text models
library(wordcloud) #Word Clouds
library(RColorBrewer) 
```


## Loading data and preparing corpus
```{r, warning=FALSE}
file.txt <- "./final/en_Us/en_US.twitter.txt"
con <- file(file.txt, "r")
twitter_corpus <- readLines(con)
close(con)
length(twitter_corpus)
set.seed(22)
selection <- sample(1:length(twitter_corpus), 1000)
twitter_corpus <- twitter_corpus[selection]
twitter_corpus <- Corpus(VectorSource(twitter_corpus),
                         readerControl=list(readPlain, language="en_US", load=TRUE))
inspect(twitter_corpus[1:4])
```

## Data cleaning

- transform to lowercase  
- remove punctuation and symbols  
- stem the corpus  
- filter profanities  
- remove stopwords 
- remove extra white spaces  
- create term document matrix and 2-gram and 3-gram tables 

```{r}
twitter_corpus <- tm_map(twitter_corpus, content_transformer(tolower))
twitter_corpus <- tm_map(twitter_corpus, removePunctuation)
twitter_corpus <- tm_map(twitter_corpus, stemDocument)
```

```{r}
set.seed(123)
profanity_list <- unique(lexicon::profanity_banned) #List containing stop words
twitter.profanity <- sentimentr::profanity(sapply(twitter_corpus, as.character), 
                                    profanity_list=profanity_list)
twitter.profanity %>% ggplot(aes(x=profanity))+
            geom_density()
```


```{r}
twitter_corpus <- tm_map(twitter_corpus, removeWords, profanity_list)
twitter.profanity <- sentimentr::profanity(sapply(twitter_corpus[s], as.character), 
                                    profanity_list=profanity_list)
mean(twitter.profanity$profanity)
```

```{r}
twitter_corpus <- tm_map(twitter_corpus, stripWhitespace)
```


```{r}
twitter_2_grams <- tokenize_ngrams(sapply(twitter_corpus, as.character), 
                                   n=2)
twitter_2_grams <- unlist(twitter_2_grams)
twitter_2_grams[1:5]
twitter_2_grams <- data.frame(table(twitter_2_grams))
twitter_2_grams <- twitter_2_grams %>% arrange(desc(Freq))
head(twitter_2_grams)
```

```{r}
twitter_2_grams <- tokenize_ngrams(sapply(twitter_corpus, as.character), 
                                   n=2, stopwords = stopwords("en"))
twitter_2_grams <- unlist(twitter_2_grams)
twitter_2_grams <- data.frame(table(twitter_2_grams))
twitter_2_grams <- twitter_2_grams %>% arrange(desc(Freq))
head(twitter_2_grams)
```

```{r}
twitter_3_grams <- tokenize_ngrams(sapply(twitter_corpus, as.character), 
                                   n=3, stopwords = stopwords("en"))
twitter_3_grams <- unlist(twitter_3_grams)
twitter_3_grams <- data.frame(table(twitter_3_grams))
twitter_3_grams <- twitter_3_grams %>% arrange(desc(Freq))
head(twitter_3_grams, 10)
```
```{r}
my_stop_words <- c("â", "œ", "ðÿ")
twitter_corpus <- tm_map(twitter_corpus, removeWords, my_stop_words)

twitter_2_grams <- tokenize_ngrams(sapply(twitter_corpus, as.character), 
                                   n=2, stopwords = stopwords("en"))
twitter_2_grams <- unlist(twitter_2_grams)
twitter_2_grams <- data.frame(table(twitter_2_grams))
twitter_2_grams <- twitter_2_grams %>% arrange(desc(Freq))
head(twitter_2_grams, 10)

twitter_3_grams <- tokenize_ngrams(sapply(twitter_corpus, as.character), 
                                   n=3, stopwords = stopwords("en"))
twitter_3_grams <- unlist(twitter_3_grams)
twitter_3_grams <- data.frame(table(twitter_3_grams))
twitter_3_grams <- twitter_3_grams %>% arrange(desc(Freq))
head(twitter_3_grams, 10)
```


```{r}
twitter_corpus <- tm_map(twitter_corpus, removeWords, stopwords("english"))
twitter_corpus <- tm_map(twitter_corpus, stripWhitespace)
inspect(twitter_corpus[1:4])
```

```{r}
twitter.dtm <- DocumentTermMatrix(twitter_corpus)
```


```{r}
file.txt <- "./final/en_Us/en_US.news.txt"
con <- file(file.txt, "r")
news_corpus <- readLines(con)
close(con)
length(news_corpus)
set.seed(23)
selection <- sample(1:length(news_corpus), 1000)
news_corpus <- news_corpus[selection]
news_corpus <- Corpus(VectorSource(news_corpus),
                      readerControl = list(readPlain, language="en_US", load=TRUE))
```

```{r}
news_corpus <- tm_map(news_corpus, content_transformer(tolower))
news_corpus <- tm_map(news_corpus, removePunctuation)
news_corpus <- tm_map(news_corpus, stemDocument)
news_corpus <- tm_map(news_corpus, removeWords, profanity_list)
news_corpus <- tm_map(news_corpus, removeWords, my_stop_words)
news_corpus <- tm_map(news_corpus, removeWords, stopwords("english"))
news_corpus <- tm_map(news_corpus, stripWhitespace)

news_2_grams <- tokenize_ngrams(sapply(news_corpus, as.character), 
                                   n=2, stopwords = stopwords("en"))
news_2_grams <- unlist(news_2_grams)
news_2_grams <- data.frame(table(news_2_grams))
news_2_grams <- news_2_grams %>% arrange(desc(Freq))

news_3_grams <- tokenize_ngrams(sapply(news_corpus, as.character), 
                                   n=3, stopwords = stopwords("en"))
news_3_grams <- unlist(news_3_grams)
news_3_grams <- data.frame(table(news_3_grams))
news_3_grams <- news_3_grams %>% arrange(desc(Freq))

news.dtm <- DocumentTermMatrix(news_corpus)
```

```{r}
file.txt <- "./final/en_Us/en_US.blogs.txt"
con <- file(file.txt, "r")
blogs_corpus <- readLines(con)
close(con)
length(blogs_corpus)
set.seed(1515)
s <- sample(1:length(blogs_corpus), 100)
mean(sapply(blogs_corpus[s], nchar))
set.seed(24)
selection <- sample(1:length(blogs_corpus), 1000)
blogs_corpus <- blogs_corpus[selection]
blogs_corpus <- Corpus(VectorSource(blogs_corpus), 
                       readerControl = list(readPlain, language="en_US", load=TRUE))
```

```{r}
blogs_corpus <- tm_map(blogs_corpus, content_transformer(tolower))
blogs_corpus <- tm_map(blogs_corpus, removePunctuation)
blogs_corpus <- tm_map(blogs_corpus, stemDocument)
blogs_corpus <- tm_map(blogs_corpus, removeWords, profanity_list)
blogs_corpus <- tm_map(blogs_corpus, removeWords, my_stop_words)
blogs_corpus <- tm_map(blogs_corpus, removeWords, stopwords("english"))
blogs_corpus <- tm_map(blogs_corpus, stripWhitespace)

blogs_2_grams <- tokenize_ngrams(sapply(blogs_corpus, as.character), 
                                   n=2, stopwords = stopwords("en"))
blogs_2_grams <- unlist(blogs_2_grams)
blogs_2_grams <- data.frame(table(blogs_2_grams))
blogs_2_grams <- blogs_2_grams %>% arrange(desc(Freq))

blogs_3_grams <- tokenize_ngrams(sapply(blogs_corpus, as.character), 
                                   n=3, stopwords = stopwords("en"))
blogs_3_grams <- unlist(blogs_3_grams)
blogs_3_grams <- data.frame(table(blogs_3_grams))
blogs_3_grams <- blogs_3_grams %>% arrange(desc(Freq))

blogs.dtm <- DocumentTermMatrix(blogs_corpus)
```

## EDA

- Wordclouds of each corpus  
- Plot of most frequent words and most frequent 2-grams and 3-grams  
- Association of most frequent words  
- Distribution of most frequent words 

```{r}
twitter.m <- as.matrix(twitter.dtm)
twitter.f <- colSums(twitter.m)
twitter.d <- data.frame(word = colnames(twitter.m), freq = twitter.f)
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
text(x=0.5, y=0.5, "Most frequent words in Twittwer corpus")
wordcloud(words = twitter.d$word, freq = twitter.d$freq, main ="Title", min.freq = 10,
          max.words = 200, random.order = F, rot.per = 0.35,
          colors = brewer.pal(8, "Set1"),
          scale = c(2.5,0.7))
```

```{r}
news.m <- as.matrix(news.dtm)
news.f <-colSums(news.m)
news.d <- data.frame(word=colnames(news.m), freq=news.f)
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
text(x=0.5, y=0.5, "Most frequent words in News corpus")
wordcloud(words = news.d$word, freq = news.d$freq, main="Title", min.freq = 10,
          max.words = 200, random.order = F, rot.per = 0.35,
          colors = brewer.pal(8, "Set1"),
          scale = c(2.5,0.7))
```

```{r}
blogs.m <- as.matrix(blogs.dtm)
blogs.f <- colSums(blogs.m)
blogs.d <- data.frame(word=colnames(blogs.m), freq=blogs.f)
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
text(x=0.5, y=0.5, "Most frequent words in news corpus")
wordcloud(words = blogs.d$word, freq = blogs.d$freq, main="Title", min.freq = 10,
          max.words = 200, random.order = F, rot.per = .35,
          colors = brewer.pal(8, "Set1"),
          scale = c(2.5,0.7))
```

